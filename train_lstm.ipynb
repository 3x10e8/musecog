{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "from datetime import date, datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "from dataloader import PianoRollDataset\n",
    "from models import LSTM\n",
    "from utils.midi_processing import convert_midi_to_piano_roll\n",
    "from utils.visuals import visualize_sequence, make_video\n",
    "from utils.export import export_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert midi dataset to piano roll\n",
    "\n",
    "This cell converts midi files to piano rolls, to save computation time during training. Make sure to prepare ```train```, ```validation``` and ```test``` subsets before running this.\n",
    "\n",
    "The time sampling of the piano rolls must be specified when converting the files, but they can be downsampled when they are loaded later on. The ```dataset_fs``` value should be equal or superior to the desired time sampling (in Hz).\n",
    "\n",
    "Run this cell even if the data are already preprocessed, to define the ```path``` and  ```dataset_fs``` variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_path = './data/midi_dataset_example/'  # Path to the MIDI dataset, containing train, validation, and test subfolders\n",
    "pr_path = './data/pr_dataset/'              # Path where the dataset will be stored with a piano roll format\n",
    "\n",
    "dataset_fs = 30                             # Sampling frequency used to convert MIDI files to piano roll format. The piano rolls\n",
    "                                            # can be automatically downsampled when they're preloaded before the training loop.\n",
    "\n",
    "if not os.path.exists(pr_path):\n",
    "    subsets = ['train/', 'validation/', 'test/']\n",
    "    for subset in subsets:\n",
    "        print(f'Converting MIDI files to piano roll format for the {subset} subset...')\n",
    "        os.makedirs(pr_path + subset, exist_ok=True)\n",
    "        convert_midi_to_piano_roll(data_path = midi_path + subset, out_dir = pr_path + subset, \n",
    "                                    fs = dataset_fs, pedal_threshold = 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset\n",
    "\n",
    "To save some computation time when using large training datasets, it is recommanded to load the data in RAM with ```preload = True```. This may take several minutes.\n",
    "\n",
    "\n",
    "The ```model_fs``` defines the time sampling (in Hz) of the data that the model will receive as input, and thus determines the temporal resolution of the model itself. This value can be inferior or equal to ```dataset_fs```.\n",
    "\n",
    "The ```ons_value``` and ```sus_value``` define the numerical value that indicate the onset or the sustain of a note in the model's inputs. Changing these values should have little impact on the model's performance. An absence of note is always indicated as ```0```.\n",
    "\n",
    "The ```max_seq_length``` set the maximum length of the input of the model (in timesteps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = pr_path                  # Path to the dataset in piano roll format.\n",
    "preload = True                          # If True, the entire dataset is loaded into memory. This will speed up the training process,\n",
    "                                        # but takes some time at loading, and requires a large amount of RAM.\n",
    "\n",
    "model_fs = 20                           # Sampling frequency of the model. This value can only be equal or lower \n",
    "                                        # than the sampling frequency of the dataset.\n",
    "batch_size = 16                         # Number of sequences processed in parallel by the model during training.\n",
    "ons_value = 1                           # Value indicating the onset of a note in the model's input.\n",
    "sus_value = 0.5                         # Value indicating the sustain of a note in the model's input.\n",
    "padding_value = -99                     # Value used to pad the input sequences to the same length.\n",
    "max_seq_length = 60*model_fs            # Maximum length of the input sequence in timesteps.\n",
    "\n",
    "train_dataset = PianoRollDataset(data_path = dataset_path + 'train/', \n",
    "               dataset_fs = dataset_fs, model_fs = model_fs, \n",
    "               ons_value = ons_value, sus_value = sus_value, \n",
    "               padding_value = padding_value,\n",
    "               source_length = max_seq_length,\n",
    "               use_transposition = True,\n",
    "               preload = preload, device = device, dtype = torch.float32)\n",
    "\n",
    "validation_dataset = PianoRollDataset(data_path = dataset_path + 'validation/', \n",
    "               dataset_fs = dataset_fs, model_fs = model_fs, \n",
    "               ons_value = ons_value, sus_value = sus_value, \n",
    "               padding_value = padding_value,\n",
    "               source_length = max_seq_length,\n",
    "               use_transposition = True,\n",
    "               preload = preload, device = device, dtype = torch.float32)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn = train_dataset.collate_batch,\n",
    "                        shuffle=True, num_workers=0)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, collate_fn = validation_dataset.collate_batch,\n",
    "                        shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instanciate model\n",
    "\n",
    "All the usual LSTM and optimizer parameters are listed below (see pytorch documentation).\n",
    "\n",
    "If ```run_name = None```, a new folder will be created to store the checkpoints during training.\n",
    "To resume the training of a model, set ```run_name``` to the name of its folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = None     #if None, create a new run folder (named with the current date and time) and start training from scratch\n",
    "                    #else, set the name of the folder to resume training from (e.g. 'lstm_run_Oct20_20-32-25')\n",
    "\n",
    "#model parameters\n",
    "input_size = 88             #dimension of the input sequence. In this case, it is the number of piano keys\n",
    "output_size = 88            #dimension of the output sequence\n",
    "\n",
    "n_lstm_layers = 1           #number of LSTM layers\n",
    "hidden_dim = 128            #dimension of the hidden state of the LSTM\n",
    "trunc_tw = 5*model_fs       #truncated backpropagation through time (BPTT) length. This will affect the ability of the model to learn long-term dependencies.\n",
    "                            #A large value will allow the model to learn dependencies over longer time scales, but the model may fail to learn anything.\n",
    "                            #A small value will restrict the learning to short-term dependencies, but the model has a higher chance of learning effectively.\n",
    "dropout = 0.1               #dropout rate\n",
    "\n",
    "#model initialization\n",
    "model = LSTM(input_size, n_lstm_layers, hidden_dim, output_size, \n",
    "                 dropout, padding_value, device)\n",
    "model.to(device)\n",
    "\n",
    "#optimizer\n",
    "lr = 1e-4               #learning rate\n",
    "betas = (0.9, 0.98)     #betas for the Adam optimizer\n",
    "eps = 1e-9              #epsilon for the Adam optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, betas=betas, eps=eps)\n",
    "\n",
    "#load model and optimizer state from the last checkpoint or start from scratch\n",
    "if run_name is None:\n",
    "    i_batch = 0\n",
    "\n",
    "    date_run = date.today().strftime('%b%d') + '_' + datetime.now().strftime('%H-%M-%S')\n",
    "    run_path = './runs/lstm_run_' + date_run + '/'\n",
    "    if os.path.exists(run_path) == False:\n",
    "        os.makedirs(run_path, exist_ok=True)\n",
    "    writer = SummaryWriter(log_dir = run_path)                # Create a tensorboard writer to store training/validation losses, \n",
    "                                                              # and plot dynamic graphs during training (see tensorboard's documentation).\n",
    "    loss_history = {'train_loss': [], 'validation_loss': []}  # Custom history of losses. Tensorboard's writer is efficient, but can\n",
    "                                                              # occasionally produce corrupted files. This custom history is a safety measure. \n",
    "\n",
    "else:\n",
    "    run_path = './runs/' + run_name + '/'\n",
    "    tmp = []\n",
    "    for file in os.listdir(run_path):\n",
    "        if 'model_' in file:\n",
    "            i_batch = int(file.split('_')[1].split('.')[0])\n",
    "            tmp.append(i_batch)\n",
    "    i_batch = max(tmp)\n",
    "    \n",
    "    writer = SummaryWriter(run_path)\n",
    "    loss_history = np.load(run_path + 'loss_history.npy', allow_pickle=True).item()\n",
    "\n",
    "    model_path = run_path + 'model_' + str(i_batch) + '.pt'\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    optimizer_path = run_path + 'optimizer_' + str(i_batch) + '.pt'\n",
    "    optimizer.load_state_dict(torch.load(optimizer_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "When running the training loop, the training and validation losses will be stored with a TensordBoard writer, and can be visualized by lauching a TensorBoard session. Since TensorBoard can sometimes be unreliable, the loss values are also stored in the ```loss_history``` dictionnary.\n",
    "\n",
    "The model state will be automatically saved during training, from every 1K batches at the beginning to every 25K batches later on.\n",
    "\n",
    "The training loop can be manually stopped and resumed at any time. The next cell allows you to save the model manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batch = 1500000\n",
    "\n",
    "# The training loop can be manually stopped and resumed. Use the next cell to manually save the model state.\n",
    "while i_batch <= n_batch:\n",
    "    for (src, tgt) in train_loader:\n",
    "        model.train()\n",
    "\n",
    "        t = 0\n",
    "        loss_batch = []\n",
    "        (h,c) = model.init_hidden(batch_size = src.shape[0])\n",
    "        while t < src.shape[1]:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            step = min(trunc_tw, src.shape[1] - t)\n",
    "            src_trunc = src[:,t:t+step,:]\n",
    "            tgt_trunc = tgt[:,t:t+step,:]\n",
    "\n",
    "            output, (h,c) = model(src_trunc, (h,c))\n",
    "            loss = model.criterion(output, tgt_trunc)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            h,c = h.detach_(), c.detach_()\n",
    "\n",
    "            loss_batch.append(loss.item())\n",
    "            t += step\n",
    "\n",
    "        #tensorboard\n",
    "        training_loss = np.mean(loss_batch)\n",
    "        if i_batch % 10 == 0:\n",
    "            writer.add_scalar('Loss/train', training_loss, i_batch)\n",
    "            loss_history['train_loss'].append((training_loss, i_batch))\n",
    "\n",
    "        print(f\"Batch: {i_batch}, Loss: {np.round(loss.item(),4)}\")\n",
    "\n",
    "        #validation loop\n",
    "        if i_batch % 100 == 0:\n",
    "            model.eval()\n",
    "            (h,c) = model.init_hidden(batch_size = src.shape[0])\n",
    "            with torch.no_grad():\n",
    "                src, tgt = next(iter(validation_loader))\n",
    "                output, (h,c) = model(src, (h,c))\n",
    "                loss = model.criterion(output, tgt)\n",
    "                \n",
    "            \n",
    "            validation_loss = copy.copy(loss.item())\n",
    "            writer.add_scalar('Loss/validation', validation_loss, i_batch)\n",
    "            loss_history['validation_loss'].append((validation_loss, i_batch))\n",
    "            print(f\"Batch: {i_batch}\")\n",
    "            print(f\" --- Training loss: {np.round(training_loss,4)}\")\n",
    "            print(f\" --- Validation loss: {np.round(validation_loss,4)}\")\n",
    "\n",
    "            #visual representation of model output to tensorboard\n",
    "            if i_batch % 1000 == 0:\n",
    "                max_t = 25*model_fs\n",
    "                fig = visualize_sequence(src[0,:max_t,:], tgt[0,:max_t,:], output[0,:max_t,:])\n",
    "                writer.add_figure('Visual/Validation', fig, i_batch)\n",
    "            \n",
    "            model.train()\n",
    "\n",
    "        #save model\n",
    "        if (i_batch % 1000 == 0 and i_batch < 10000) or (i_batch % 5000 == 0 and i_batch < 100000) or (i_batch % 10000 == 0 and i_batch < 300000) or (i_batch % 25000 == 0 and i_batch > 300000):\n",
    "            torch.save(model.state_dict(), run_path + f'/model_{i_batch}.pt')\n",
    "            torch.save(optimizer.state_dict(), run_path + f'/optimizer_{i_batch}.pt')\n",
    "            np.save(run_path + 'loss_history.npy', loss_history)\n",
    "\n",
    "            print(f'Model saved at batch {i_batch}')\n",
    "\n",
    "        i_batch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to save manually, run this cell\n",
    "torch.save(model.state_dict(), run_path + f'/model_{i_batch}.pt')\n",
    "torch.save(optimizer.state_dict(), run_path + f'/optimizer_{i_batch}.pt')\n",
    "print(f'Model saved at batch {i_batch}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select best model\n",
    "\n",
    "This cell computes the validation loss over the entire validation set for each checkpoint of the model, and selects the model state with the lowest validation loss.\n",
    "\n",
    "To evaluate the model on a subsets of the checkpoints only, set the ```versions_range``` to the desired i_batch range.\n",
    "\n",
    "To speed up the process, the validation set can be arbitrarely divided by ```n```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "versions_range = [0, n_batch]  #range of model versions to evaluate\n",
    "n = 1 #divide by n the validation set to speed up the process\n",
    "\n",
    "#retrieve model versions to evaluate\n",
    "checkpoints = []\n",
    "for file in os.listdir(run_path):\n",
    "    if 'model_' in file:\n",
    "        i_batch = int(file.split('_')[1].split('.')[0])\n",
    "        if i_batch >= versions_range[0] and i_batch <= versions_range[1]:\n",
    "            checkpoints.append(i_batch)\n",
    "checkpoints = np.sort(checkpoints).tolist()\n",
    "print(f\"Evaluating checkpoints: {checkpoints}\")\n",
    "\n",
    "#compute loss on entire validation set for each model version (this can take some time)\n",
    "val_loss = []\n",
    "for checkpoint in checkpoints:\n",
    "    tmp = []\n",
    "    model_path = run_path + 'model_' + str(checkpoint) + '.pt'\n",
    "    model.load_state_dict(torch.load(model_path))       #make sure that the model has been instanciated before running this\n",
    "    model.eval()\n",
    "\n",
    "    i = 0\n",
    "    for (src, tgt) in validation_loader:\n",
    "        if i % n == 0: \n",
    "            (h,c) = model.init_hidden(batch_size = src.shape[0])\n",
    "            with torch.no_grad():\n",
    "                output, (h,c) = model(src, (h,c))\n",
    "                loss = model.criterion(output, tgt)\n",
    "                tmp.append(loss.item())\n",
    "        i += 1\n",
    "    print(f\"Batch: {checkpoint}, Validation loss: {np.mean(tmp)}\")\n",
    "    val_loss.append(np.mean(tmp))\n",
    "\n",
    "#retrieve best model\n",
    "print('---')\n",
    "best_model_checkpoint = checkpoints[np.argmin(val_loss)]\n",
    "best_train_loss = loss_history['train_loss'][np.argmin([np.abs(s - best_model_checkpoint) for s in [s for (_, s) in loss_history['train_loss']]])][0]\n",
    "best_val_loss = val_loss[np.argmin(val_loss)]\n",
    "print(f\"Best model: model_{best_model_checkpoint}\")\n",
    "print(f\"Training loss: {best_train_loss}\")\n",
    "print(f\"Validation loss: {best_val_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save final version\n",
    "\n",
    "The final version of a model is saved in ```'./versions/model_name/```, in a ```model.pt``` file. It can be loaded in other scripts with ```torch.load()```.\n",
    "\n",
    "All the parameters to instanciate, evaluate or fine-tune the model are saved in ```info.pt```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'lstm_maestro_test'\n",
    "additional_info = ''' Examplar model, trained on a subset of the MAESTRO 3.0 dataset. '''\n",
    "\n",
    "os.makedirs('./versions/' + name, exist_ok=True)\n",
    "\n",
    "model_path = run_path + 'model_' + str(best_model_checkpoint) + '.pt'\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "optimizer_path = run_path + 'optimizer_' + str(best_model_checkpoint) + '.pt'\n",
    "optimizer.load_state_dict(torch.load(optimizer_path))\n",
    "\n",
    "if os.path.exists('./versions/' + name + '/model.pt'):\n",
    "    print('A model already exists at this location !')\n",
    "else:\n",
    "    torch.save(model, './versions/' + name + '/model.pt')\n",
    "    torch.save({'model_type': 'lstm',\n",
    "                'seed': seed,\n",
    "                'i_batch': best_model_checkpoint,\n",
    "                'fs': model_fs,\n",
    "                'ons_value': ons_value,\n",
    "                'sus_value': sus_value,\n",
    "                'padding_value': padding_value,\n",
    "                'batch_size': batch_size,\n",
    "                'input_size' : input_size,\n",
    "                'output_size': output_size,\n",
    "                'hidden_dim': hidden_dim,\n",
    "                'n_lstm_layers': n_lstm_layers,\n",
    "                'trunc_tw': trunc_tw,\n",
    "                'max_seq_length': max_seq_length,\n",
    "                'dropout': dropout,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': best_train_loss,\n",
    "                'val_loss': best_val_loss,\n",
    "                'additional_info': additional_info,\n",
    "                }, \n",
    "                './versions/' + name + '/info.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize with video\n",
    "\n",
    "The ```make_video()``` function requires fluidsynth and ffmpeg to work. Links are available on https://github.com/pl-robert/musecog.\n",
    "\n",
    "The video will be saved in ./versions/model_name/video/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_video(file_path = './data/midi_dataset_example/visualization/',\n",
    "            file_name = 'MIDI-Unprocessed_059_PIANO059_MID--AUDIO-split_07-07-17_Piano-e_2-03_wav--1.mid',\n",
    "            res = (1280,720), \n",
    "            graph = True,\n",
    "            model_name = 'lstm_maestro_test',\n",
    "            ffmpeg_path = r'C:/ffmpeg/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export features\n",
    "\n",
    "The ```export_features()``` function allows you to compute and save a series of features of interest from a given model and midi dataset.\n",
    "\n",
    "The ```out_fs``` parameter defines the time sampling of time-resolved features. Even if a model runs at 20Hz, the features can be upsampled to align with continuous neural or behavioral data of interest.\n",
    "\n",
    "If ```timing_correction = True```, the small temporal misalignements caused by a low ```model_fs``` are corrected to match the exact note onset timings (from the midi files), at the cost of minor distorsion in the features values (linear interpolation).\n",
    "\n",
    "The following features are computed at each timestep:\n",
    "- **surprise:** Binary Cross Entropy (BCE) between the model's predictions and the target values\n",
    "- **surprise_max:** maximum BCE across all simulatenous notes\n",
    "- **surprise_scaled:** BCE scaled by the number of simultaneous notes in the target\n",
    "- **surprise_positive:** positive part of the BCE\n",
    "- **surprise_positive_max:** maximum positive part of the BCE across all simulatenous notes\n",
    "- **surprise_positive_scaled:** positive part of the BCE scaled by the number of simultaneous notes in the target\n",
    "- **surprise_negative:** negative part of the BCE\n",
    "- **surprise_negative_max:** maximum negative part of the BCE across all absent notes\n",
    "- **surprise_negative_scaled:** negative part of the BCE scaled by the number of absent notes in the target\n",
    "- **uncertainty:** entropy of the model's predictions. The probabilities are normalized (sum = 1) before computing the entropy. \n",
    "- **predicted_density:** predicted note density (sum of probabilities) \n",
    "\n",
    "They are stored in the files:\n",
    "- **features.csv:** contains a summary of the features of all midi files in a single table. Each feature is summed over time, either\n",
    "                across all timesteps ('continuous_xxx' features) or only at the timesteps with note onsets ('onsets_xxx' features). In\n",
    "                addition, the table contains the number of notes, events (group of simulatenous notes) and the duration of each file.\n",
    "- **features_over_time:** folder containing the time-resolved features for each midi file. Each file contain all features, with the \n",
    "                addition of a time axis and a binary mask for the note onsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_features(data_path = './my_stimuli/midi/',\n",
    "           output_path = './my_stimuli/',\n",
    "           model_name = 'lstm_maestro_test',\n",
    "           out_fs = 100,\n",
    "           timing_correction = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
